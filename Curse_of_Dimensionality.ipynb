{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Columns of the data set is called features\n",
        "and features is only called dimensions therefore its also called feature dimensionality.\n",
        "\n",
        "There is only optimal number of feature (fn) for a model and on adding more features to the optimal number the model won't increase its efficiency or performance rather it may result in reduce in accuracy score due to additional number of features.\n",
        "\n",
        "This is called curse of dimensionality(features).\n",
        "\n",
        "Image and text data set has high dimensionality.\n",
        "\n",
        "Higher dimension increases sparsity(the area or the concered dimension) as the area increased computation to find something in that hight dimensional space increses therefore accuracy decreases.\n",
        "\n",
        "eg: find a point in a set of 4 point in 1 dimension space and the other is finding the same point in a set of 25 points and that too in a 3 dimension space.\n",
        "\n",
        "To overcome this only we have dimensionality reduction\n",
        "\n",
        "\n",
        "\n",
        "*   Feature/Dimension Selection\n",
        "\n",
        "1.   Forward Selection\n",
        "2.   Backward Elimination\n",
        "\n",
        "\n",
        "*   Feature Extraction\n",
        "\n",
        "1.   PCA (Pricipal Component Analysis)\n",
        "2.   LDA (Linear Discriminate Analysis)\n",
        "3.   tsne\n",
        "\n",
        "Conclusion: Higher Dimensional data is dangerous for our model because after certain point working with higher dimensional data is not possible\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lROdDZWIc3Ns"
      }
    }
  ]
}